{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1342747a",
   "metadata": {},
   "source": [
    "### Understanding Frontier models\n",
    "\n",
    "- Closed-source Frontier - models you have to pay for\n",
    "  - GPT from OpenAI - US\n",
    "  - Claude fromm Anthropic - US\n",
    "  - Gemini from Google - US\n",
    "  - Grok from X - US\n",
    "  - Command R from Cohere - CA\n",
    "  - Mixtral from mistral - FR\n",
    "  - Perplexity - US\n",
    "  \n",
    "- Open-source Frontier\n",
    "  - Llama from meta 3.2, 4.0\n",
    "  - Mixtral from mistral\n",
    "  - Qwen from Alibaba Cloud - Chinese\n",
    "  - Gemma from Google\n",
    "  - Phi from Microsoft Phi4\n",
    "  - DeepSeek from DeepSeek AI\n",
    "  - GPT-OSS from OpenAI\n",
    "\n",
    "- Three ways to use models: you make code that call llm\n",
    "  - Chat interfaces (like ChatGPT)\n",
    "  - Cloud APIs (LLM API) \n",
    "    - Frameworks like LangChain\n",
    "    - Managed AI cloud services: Amazon Bedrock, Google Vertex, Azure ML, Grok, Open rooter \n",
    "  - Direct inference:\n",
    "    - HuggingFace Transformers library\n",
    "    - With Ollama to run locally"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

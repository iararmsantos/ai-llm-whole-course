{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6b55be",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "- 2017 - \"Attention is all you need\" - Google scientist\n",
    "  - They didn't know what the progress the would case to the AI field\n",
    "- Prompt engineers\n",
    "- Custom GPTs\n",
    "- Copilots - collaborative work\n",
    "- Context Engineering - all the information you can give to the LLM in the prompt, Tools\n",
    "- Agentization - collaboration between llms - Claude Code, Cursor Agents\n",
    "- Autonomy - AI being able to choose its destiny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33942a90",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "- Number of parameters in models:\n",
    "    - GPT-1 (2017): 117M\n",
    "    - GPT-2: 1.5B\n",
    "    - GPT-3: 175B\n",
    "    - GPT-4: 1.76T\n",
    "    - GPT-5 (2025) is not public, but estimates range from a minimum of 5 trillion to potentially 50 trillion or more\n",
    "    - Sometimes we do more with less. So less parameter sometimes show better results\n",
    "\n",
    "- Training time scaling: a model with more parameters is more expensive to training it\n",
    "    - more parameters means we can training it more\n",
    "- Inference time scaling: performing better at inference time:\n",
    "    - reasoning (discover how AI answer a question)\n",
    "    - more information in the input sequence\n",
    "\n",
    "- Number of parameters in open source models:\n",
    "    - Llama 3.2: 3B\n",
    "    - Llama 3.1: 8B\n",
    "    - Llama 3.3: 70B\n",
    "    - GPT-OSS: 120B\n",
    "    - DeepSeek: 671B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ca52d",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "  - neural networks were trained at the character level: predict the next character in this sequence\n",
    "    - small vocab, but expects too much from network\n",
    "  - then, neural networks were trained off words: predict the next word in this sequence\n",
    "    - much easier to learn from, but enormous vocabs with rare words omitted\n",
    "  - then, work with chunks of words, called tokens\n",
    "    - manageable vocab, and useful information for neural network - handles word stems - partially words that can mean another words\n",
    "  - GPT's Tokenizer example: \n",
    "    - https://platform.openai.com/tokenizer -> tool to understand tokenizer\n",
    "    - 'An important sentence for my class of AI engineers' -> 9 tokens 50 characters\n",
    "    - words with spaces means part of tokens\n",
    "\n",
    "  <img src=\"../images/gptTokenizer.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4886e7",
   "metadata": {},
   "source": [
    "### Context Window\n",
    "- max number of tokens that the model consider when generating the next token\n",
    "- total conversation until the next token it is predicting\n",
    "- how well the model can remember references, content, and context\n",
    "  \n",
    "### API costs\n",
    "- does not have a monthly subscription, you pay by call\n",
    "- gpt-mini $0.15/1M tokens\n",
    "- gpt5-nano $0.05/1M tokens"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb59b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e28ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79937a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad21b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9dab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many recent turns you want to keep verbatim (e.g. last 3 turns)\n",
    "summary_cutoff = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d419a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini, Gemini and Claude-3-haiku, and gemini-2.0-flash\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model=\"gemini-2.5-flash-preview-04-17\",\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_system = \"You are optimistic, funny chatbot. You try to make fun of everything the other person say, without ridicule, just rty to make the other have fun as well.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"What's up\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99cb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation():\n",
    "    summary_messages = []\n",
    "    for gpt, claude, gemini in zip(\n",
    "        gpt_messages[:-summary_cutoff],\n",
    "        claude_messages[:-summary_cutoff],\n",
    "        gemini_messages[:-summary_cutoff]\n",
    "    ):\n",
    "        summary_messages.append(f\"GPT: {gpt}\\nClaude: {claude}\\nGemini: {gemini}\")\n",
    "\n",
    "    full_history = \"\\n---\\n\".join(summary_messages)\n",
    "\n",
    "    summary_prompt = f\"\"\"\n",
    "Summarize the following multi-agent conversation between GPT, Claude, and Gemini. \n",
    "Keep it brief, only include key ideas or themes.\n",
    "\n",
    "Conversation:\n",
    "{full_history}\n",
    "\"\"\"\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a summarizer for multi-agent AI conversations.\"},\n",
    "            {\"role\": \"user\", \"content\": summary_prompt}\n",
    "        ],\n",
    "        max_tokens=250\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f877594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified plain-text streaming interface that works in any Python environment\n",
    "def stream_model_response(name, stream, chunk_getter, display=True):\n",
    "    if display:\n",
    "        print(f\"\\n{name}:\\n\", end=\"\", flush=True)\n",
    "    full_response = \"\"\n",
    "    if not display:\n",
    "        yield f\"{name}:\\n\"\n",
    "    for chunk in stream:\n",
    "        text = chunk_getter(chunk)\n",
    "        if text:\n",
    "            full_response += text\n",
    "            if display:\n",
    "                print(text, end=\"\", flush=True)\n",
    "            if not display:\n",
    "                yield text\n",
    "    if not display:           \n",
    "        yield \"\\n\"\n",
    "\n",
    "    if display:\n",
    "        print(\"\\n\")  # End with newline for clean formatting\n",
    "    return full_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f76fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude(display=True):\n",
    "    messages = []\n",
    "\n",
    "    if len(gpt_messages) > summary_cutoff:\n",
    "        summary = summarize_conversation()\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"(Summary of previous conversation):\\n{summary}\"})\n",
    "\n",
    "    for gpt, gemini, claude_message in zip(\n",
    "        gpt_messages[-summary_cutoff:],\n",
    "        gemini_messages[-summary_cutoff:],\n",
    "        claude_messages[-summary_cutoff:]\n",
    "    ):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"GPT: {gpt}\\nGemini: {gemini}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "\n",
    "    # Keep conversation going with latest input\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"GPT: {gpt_messages[-1]}\\nGemini: {gemini_messages[-1]}\"})\n",
    "\n",
    "    stream = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        stream=True  # Required for streaming\n",
    "    )\n",
    "\n",
    "    return stream_model_response(\"Claude\", stream, lambda e: e.delta.text if e.type == \"content_block_delta\" else \"\", display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(display=True):\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "\n",
    "    if len(gpt_messages) > summary_cutoff:\n",
    "        summary = summarize_conversation()\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"(Summary of previous conversation):\\n{summary}\"})\n",
    "\n",
    "    for claude, gemini, gpt in zip(\n",
    "        claude_messages[-summary_cutoff:],\n",
    "        gemini_messages[-summary_cutoff:],\n",
    "        gpt_messages[-summary_cutoff:]\n",
    "    ):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Claude: {claude}\\nGemini: {gemini}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Claude: {claude_messages[-1]}\\nGemini: {gemini_messages[-1]}\"})\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "\n",
    "    return stream_model_response(\"GPT\", stream, lambda c: c.choices[0].delta.content or \"\", display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08270e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(display=True):\n",
    "    messages = []\n",
    "\n",
    "    if len(gpt_messages) > summary_cutoff:\n",
    "        summary = summarize_conversation()\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [{\"text\": f\"(Summary of earlier conversation):\\n{summary}\"}]\n",
    "        })\n",
    "\n",
    "    # Add last few verbatim turns\n",
    "    for gpt, claude, gemini in zip(\n",
    "        gpt_messages[-summary_cutoff:],\n",
    "        claude_messages[-summary_cutoff:],\n",
    "        gemini_messages[-summary_cutoff:]\n",
    "    ):\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [{\"text\": f\"Claude: {claude}\\nGPT: {gpt}\"}]\n",
    "        })\n",
    "        messages.append({\"role\": \"assistant\", \"parts\": [{\"text\": gemini}]})\n",
    "\n",
    "    # Add latest user turn\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": f\"Claude: {claude_messages[-1]}\\nGPT: {gpt_messages[-1]}\"}]\n",
    "    })\n",
    "\n",
    "    # gemini = google.generativeai.GenerativeModel(\n",
    "    #     model_name='gemini-2.0-flash',\n",
    "    #     system_instruction=gemini_system\n",
    "    # )\n",
    "\n",
    "    # completion = gemini.generate_content(messages)\n",
    "    # return completion.text\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=gemini_system\n",
    "    )\n",
    "\n",
    "    stream = gemini.generate_content(messages, stream=True)\n",
    "\n",
    "    return stream_model_response(\"Gemini\", stream, lambda c: c.text or \"\", display)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae8065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No interface\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"What's up\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cca12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def run_conversation():\n",
    "    gpt_messages = [\"Hi there\"]\n",
    "    claude_messages = [\"Hi\"]\n",
    "    gemini_messages = [\"What's up\"]\n",
    "\n",
    "    full_output = f\"GPT:\\n{gpt_messages[0]}\\n\\nClaude:\\n{claude_messages[0]}\\n\\nGemini:\\n{gemini_messages[0]}\\n\\n\"\n",
    "    yield full_output\n",
    "\n",
    "    for i in range(5):\n",
    "        full_output += f\"Turn {i + 1}:\\n\"\n",
    "\n",
    "        # GPT stream\n",
    "        gpt_stream = call_gpt(display=False)\n",
    "        gpt_next = \"\"\n",
    "        for chunk in gpt_stream:\n",
    "            gpt_next += chunk\n",
    "            partial_turn_output = full_output + f\"GPT: {gpt_next}\\n\"\n",
    "            yield partial_turn_output\n",
    "        gpt_messages.append(gpt_next)\n",
    "        full_output += f\"{gpt_next}\\n\"\n",
    "\n",
    "        # Claude stream\n",
    "        claude_stream = call_claude(display=False)\n",
    "        claude_next = \"\"\n",
    "        for chunk in claude_stream:\n",
    "            claude_next += chunk\n",
    "            partial_turn_output = full_output + f\"Claude: {claude_next}\\n\"\n",
    "            yield partial_turn_output\n",
    "        claude_messages.append(claude_next)\n",
    "        full_output += f\"{claude_next}\\n\"\n",
    "\n",
    "        # Gemini stream\n",
    "        gemini_stream = call_gemini(display=False)\n",
    "        gemini_next = \"\"\n",
    "        for chunk in gemini_stream:\n",
    "            gemini_next += chunk\n",
    "            partial_turn_output = full_output + f\"Gemini: {gemini_next}\\n\"\n",
    "            yield partial_turn_output\n",
    "        gemini_messages.append(gemini_next)\n",
    "        full_output += f\"{gemini_next}\\n\\n\"\n",
    "        \n",
    "        time.sleep(1)  # Optional, simulate delay\n",
    "    yield full_output\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ðŸ¤– Multi-Model Conversation Viewer\")\n",
    "    start_button = gr.Button(\"Start Conversation\")\n",
    "    chat_output = gr.Textbox(label=\"Conversation\", lines=30)\n",
    "\n",
    "    start_button.click(fn=run_conversation, outputs=chat_output)\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

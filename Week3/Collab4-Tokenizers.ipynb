{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2cd7804",
   "metadata": {},
   "source": [
    "Quantization: reduce the weight of the precision of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ff3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "hf_token = os.getenv('HUGGING_FACE_CLAIRE')\n",
    "if hf_token:\n",
    "    print(f\"Hugging Face Key exists and begins {hf_token[:8]}\")\n",
    "else:\n",
    "    print(f\"Hugging Face Key does not exists.\")\n",
    "\n",
    "#login into hugging face\n",
    "login(token=hf_token, add_to_git_credential=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA= \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA = \"google/gemma-2-2b-it\"\n",
    "QWEN2 = \"Qwen/Qwen2-7B-Instruct\"\n",
    "STARCODER2 = \"bigcode/starcoder2-3b\"\n",
    "MIXTRAL = \"mistralai/Mixtral-9x7b-Instruct-v0.1\" #heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4de925",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa75fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking internet conectivity:\n",
    "import requests\n",
    "print(requests.get(\"https://huggingface.co\").status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587de3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory - reduce the precision\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ecdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model - create model, it connects to hugging face and download all the model weights and put in cache/memory, when disconnect it will be deleted\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we see how much of memory it is occupying:\n",
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0afa6",
   "metadata": {},
   "source": [
    "### Looking under the hood at the Transformer model\n",
    "The next cell prints the HuggingFace model object for Llama.\n",
    "\n",
    "This model object is a Neural Network, implemented with the Python framework PyTorch. The Neural Network uses the architecture invented by Google scientists in 2017: the Transformer architecture.\n",
    "\n",
    "While we're not going to go deep into the theory, this is an opportunity to get some intuition for what the Transformer actually is.\n",
    "\n",
    "If you're completely new to Neural Networks, check out my [YouTube intro playlist](https://www.youtube.com/playlist?list=PLWHe-9GP9SMMdl6SLaovUQF2abiLGbMjs) for the foundations.\n",
    "\n",
    "Now take a look at the layers of the Neural Network that get printed in the next cell. Look out for this:\n",
    "\n",
    "It consists of layers\n",
    "There's something called \"embedding\" - this takes tokens and turns them into 4,096 dimensional vectors. We'll learn more about this in Week 5.\n",
    "There are then 32 sets of groups of layers called \"Decoder layers\". Each Decoder layer contains three types of layer: (a) self-attention layers (b) multi-layer perceptron (MLP) layers (c) batch norm layers.\n",
    "There is an LM Head layer at the end; this produces the output\n",
    "Notice the mention that the model has been quantized to 4 bits.\n",
    "\n",
    "It's not required to go any deeper into the theory at this point, but if you'd like to, I've asked our mutual friend to take this printout and make a tutorial to walk through each layer. This also looks at the dimensions at each point. If you're interested, work through this tutorial after running the next cell:\n",
    "\n",
    "https://chatgpt.com/canvas/shared/680cbea6de688191a20f350a2293c76b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell and look at what gets printed; investigate the layers\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d666154",
   "metadata": {},
   "source": [
    "### And if you want to go even deeper into Transformers\n",
    "In addition to looking at each of the layers in the model, you can actually look at the HuggingFace code that implements Llama using PyTorch.\n",
    "\n",
    "Here is the HuggingFace Transformers repo:\n",
    "https://github.com/huggingface/transformers\n",
    "\n",
    "And within this, here is the code for Llama 4:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py\n",
    "\n",
    "Obviously it's not neceesary at all to get into this detail - the job of an AI engineer is to select, optimize, fine-tune and apply LLMs rather than to code a transformer in PyTorch. OpenAI, Meta and other frontier labs spent millions building and training these models. But it's a fascinating rabbit hole if you're interested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, with that, now let's run the model!\n",
    "# send the request to the model with maximum og 80 tokens then get the response and decode this from machine language to human language\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=80)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "# Thank you Kuan L. for helping me get this to properly free up memory!\n",
    "# If you select \"Show Resources\" on the top right to see GPU memory, it might not drop down right away\n",
    "# But it does seem that the memory is available for use by new models in the later code.\n",
    "\n",
    "del model, inputs, tokenizer, outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdbe609",
   "metadata": {},
   "source": [
    "I'm using a HuggingFace utility called TextStreamer so that results stream back. To stream results, we simply replace:\n",
    "outputs = model.generate(inputs, max_new_tokens=80)\n",
    "With:\n",
    "streamer = TextStreamer(tokenizer)\n",
    "outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "\n",
    "Also I've added the argument add_generation_prompt=True to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a73b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
    "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "  del model, inputs, tokenizer, outputs, streamer\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98099617",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(PHI3, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c168029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma doesn't support system message so we redefine message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]\n",
    "generate(GEMMA, messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
